<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>DecAlign</title>
  <link rel="icon" type="image/x-icon" href="figs\taco.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


    <section class="hero">
        <div class="hero-body">
          <div class="container is-max-desktop">
            <div class="columns is-centered">
              <div class="column has-text-centered">
                <h1 class="title is-1 publication-title">DecAlign: Hierarchical Cross-Modal Alignment for Decoupled Multimodal Representation Learning
                </h1>
                <div class="is-size-5 publication-authors">
                  <!-- Paper authors -->
                  <span class="author-block">
                    <a href="https://qiancx.com/" target="_blank">Chengxuan Qian</a><sup>1,†</sup>,</span>
                  <span class="author-block">
                    <a href="https://shuoxing98.github.io/" target="_blank">Shuo Xing</a><sup>1</sup>,</span>
                  <span class="author-block">
                    <a href="https://lili0415.github.io/" target="_blank">Shawn Li</a><sup>2</sup>,
                  </span>
                  <span class="author-block">
                    <a href="https://viterbi-web.usc.edu/~yzhao010/lab" target="_blank">Yue Zhao</a><sup>2</sup>,
                  </span>
                  <span class="author-block">
                    <a href="https://vztu.github.io/" target="_blank">Zhengzhong Tu</a><sup>1,*</sup>,
                  </span>
                </div>
                  
                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Texas A&M University, <sup>2</sup>University of Southern California</span>
                    <span class="eql-cntrb"><small><br><sup>†</sup>Work done during the internship at Texas A&M University</small></span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Corresponding author</small></span>
                  </div>
                  
                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/taco-group/DecAlign" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2502.13146" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- <video poster="" id="tree" autoplay controls muted loop height="100%"> -->
        <!-- Your video here -->
        <!-- <source src="static/videos/banner_video.mp4" -->
        <!-- type="video/mp4"> -->
      <!-- </video> -->
      <img src="figs\decalign_pip.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        The Overview of our proposed DecAlign framework. 
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Multimodal representation learning aims to capture both shared and complementary semantic information across multiple modalities. However, the intrinsic heterogeneity of diverse modalities presents substantial challenges to achieve effective cross-modal collaboration and integration. To address this, we introduce DecAlign, a novel hierarchical cross-modal alignment framework designed to decouple multimodal representations into modality-unique (heterogeneous) and modality-common (homogeneous) features. For handling heterogeneity, we employ a prototype-guided optimal transport alignment strategy leveraging gaussian mixture modeling and multi-marginal transport plans, thus mitigating distribution discrepancies while preserving modality-unique characteristics. To reinforce homogeneity, we ensure semantic consistency across modalities by aligning latent distribution matching with Maximum Mean Discrepancy regularization. Furthermore, we incorporate a multimodal transformer to enhance high-level semantic feature fusion, thereby further reducing cross-modal inconsistencies. Our extensive experiments on four widely used multimodal benchmarks demonstrate that DecAlign consistently outperforms existing state-of-the-art methods across five metrics. These results highlight the efficacy of DecAlign in enhancing superior cross-modal alignment and semantic consistency while preserving modality-unique features, marking a significant advancement in multimodal representation learning scenarios.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop content">
      <h2 class="title is-3">Method Overview</h2>
      <img src="figs\overview.png" alt="MY ALT TEXT" style="display: block; margin: 0 auto; width: 800px; height: auto;"/>
      <div class="content has-text-justified">
        <p>

        </p>
        <ul>
          <li><strong>Multimodal Representation Decoupling: </strong>DecAlign decouples multimodal features into modality-unique (heterogeneous) and modality-common (homogeneous) representations. Modality-unique encoders extract heterogeneous features to capture specific characteristics, while a modality-common encoder derives homogeneous features for common semantics. This decoupling effectively reduces cross-modal interference while simultaneously enhancing the ability to capture underlying semantic commonalities across modalities.</li>
          <li><strong>Heterogeneity Alignment: </strong>DecAlign introduces a prototype-guided optimal transport strategy to mitigate distributional discrepancies among modality-unique features, which often pose challenges to seamless cross-modal integration. By incorporating Gaussian Mixture Models (GMM), DecAlign effectively captures complex intra-modal structures, generating adaptive prototypes that act as alignment anchors. A multi-marginal optimal transport mechanism then dynamically aligns these prototypes across modalities, bridging distributional gaps while preserving modality-unique characteristics. This approach not only maintains the distinctive nuances of each modality but also facilitates a more cohesive integration into a shared multimodal representation space.</li>
          <li><strong>Homogeneity Alignment: </strong>To ensure semantic consistency and enable seamless multimodal fusion, DecAlign aligns common features through latent distribution matching and maximum mean discrepancy regularization. By systematically correcting global statistical shifts in means, covariances, and higher-order moments, DecAlign preserves the intrinsic semantic relationships of modality-common representations while mitigating distortions caused by distributional inconsistencies. This approach not only fosters effective feature integration but also enhances the model’s robustness in handling diverse multimodal scenarios.</li>
          <li><strong>Hierarchical Alignment Strategy: </strong>DecAlign bridges modality gaps by first aligning heterogeneous features through prototype-based optimal transport, then ensuring semantic coherence via latent space alignment and MMD regularization. This hierarchical approach enhances multimodal fusion accuracy and generalizability, as demonstrated in benchmark evaluations.</li>
        </ul> 
      </div>
    </div>
  </div>
</section>
<!-- End image carousel -->


<!-- Image carousel -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container is-max-desktop content">
      <h2 class="title is-3">Comparison Analysis</h2>
        <p>
          Comprehensive experiments conducted across four widely-used multimodal benchmarks demonstrate DecAlign's superior performance compared to existing state-of-the-art methods. The results consistently show that DecAlign achieves substantial improvements in both fine-grained semantic distinction and overall alignment accuracy, highlighting the effectiveness and robustness of its hierarchical alignment strategy.
        </p>
        
        <img src="figs\table1.png" alt="Comparison 1" style="display: block; margin: 0 auto; width: 1000px; height: auto;"/>
        <div class="content has-text-centered"><p></p><strong>Table 1. </strong> 
          <em>Performance Comparison on CMU-MOSI and CMU-MOSEI datasets.</em>
        </div>
        
        <img src="figs\table2.png" alt="Comparison 2" style="display: block; margin: 0 auto; width: 600px; height: auto;"/>
        <div class="content has-text-centered"><p></p><strong>Table 2. </strong> 
          <em>Performance Comparison on CH-SIMS and IEMOCAP datasets.</em>
        </div>

        <p>
          Experimental results presented in Tables 1 and 2 indicate that DecAlign consistently outperforms existing state-of-the-art multimodal methods across various datasets and evaluation metrics. Specifically, DecAlign achieves the lowest Mean Absolute Error (MAE) and the highest correlation coefficients, binary accuracies (Acc-2), and F1 Scores, demonstrating significant improvements in both regression and classification tasks.
        </p>

        <img src="figs\v1.png" alt="Comparison 3" style="display: block; margin: 0 auto; width: 1000px; height: auto;"/>
        <div class="content has-text-centered"><p></p><strong>Figure 1. </strong> 
          <em>Visualization showcasing the superior performance of our proposed DecAlign, which surpasses state-of-the-art methods across multiple multimodal benchmarks.</em>
        </div>
           
        <p>
          The bubble chart visualization further emphasizes DecAlign's balanced and superior performance, maintaining high accuracy and F1 scores across diverse multimodal benchmarks, highlighting its robustness. Additionally, the confusion matrices clearly illustrate that DecAlign significantly reduces misclassification across sentiment intensity categories, exhibiting improved diagonal dominance and an enhanced ability to distinguish nuanced sentiment classes, thereby underscoring its precise alignment and semantic understanding.
        </p>
      </div>
    </div>
  </div>
</section>
<!-- End image carousel -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop content">
      <h2 class="title is-3">Ablation Analysis</h2>
        <p>
          Ablation studies (Tables 3 and 4) systematically evaluate the impact of DecAlign’s key modules and specific alignment strategies. Table 3 demonstrates that multimodal feature decoupling (MFD), heterogeneous alignment (Hete), and homogeneous alignment (Homo) each substantially contribute to performance, with the removal of either Hete or Homo individually resulting in minor performance drops, and the absence of both causing a notable decline. This confirms their essential roles and complementary interaction. Table 4 further analyzes individual alignment techniques such as Prototype-Based Optimal Transport (Proto-OT), Contrastive Training (CT), Semantic Consistency (Sem), and Maximum Mean Discrepancy (MMD). Results show that each alignment strategy significantly influences performance, emphasizing the critical importance of both fine-grained and global alignment mechanisms within DecAlign.
        </p>
        <img src="figs/ab1.png" alt="MY ALT TEXT" style="display: block; margin: 0 auto; width: 800px; height: auto;"/>
        <div class="content has-text-centered"><p></p><strong>Table 3. </strong> 
          <em>Ablation study on different key modules for DecAlign on CMU-MOSI and CMU-MOSEI datasets.</em>
        </div>
        <img src="figs\ab2.png" alt="MY ALT TEXT" style="display: block; margin: 0 auto; width: 800px; height: auto;"/>
        <div class="content has-text-centered"><p></p><strong>Table 4. </strong> 
          <em>Ablation study on different alignment strategies for DecAlign on CMU-MOSI and CMU-MOSEI datasets.</em>
        </div>
        <p>
          Figures 2 and 4 provide additional insights into DecAlign's effectiveness. Figure 2 visually demonstrates the impact of removing heterogeneous and homogeneous alignment modules, highlighting the consistent performance degradation across different sentiment categories when either module is omitted. This underscores the necessity of both alignment strategies in maintaining robust multimodal sentiment classification performance. Figure 3 offers a visual case study on the modality gap between vision and language features, illustrating that DecAlign effectively reduces modality discrepancies through hierarchical alignment. Models without heterogeneous or homogeneous alignment exhibit significantly larger modality gaps, validating DecAlign’s capability to bridge semantic and distributional differences across modalities.
        </p>
        <img src="figs\ab3.png" alt="MY ALT TEXT" style="display: block; margin: 0 auto; width: 600px; height: auto;"/>
        <div class="content has-text-centered"><p></p><strong>Figure 2. </strong> 
          <em>Visualization of ablation studies on accuracy comparison across different emotion categories.</em>
        </div>
        <img src="figs\ab4.png" alt="MY ALT TEXT" style="display: block; margin: 0 auto; width: 600px; height: auto;"/>
        <div class="content has-text-centered"><p></p><strong>Figure 3. </strong> 
          <em>Visualization of the modality gap between vision and language on CMU-MOSEI dataset.</em>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End image carousel -->

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>