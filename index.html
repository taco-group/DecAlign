<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>DecAlign</title>
  <link rel="icon" type="image/x-icon" href="figs\taco.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


    <section class="hero">
        <div class="hero-body">
          <div class="container is-max-desktop">
            <div class="columns is-centered">
              <div class="column has-text-centered">
                <h1 class="title is-1 publication-title">DecAlign: Hierarchical Cross-Modal Alignment for Decoupled Multimodal Representation Learning
                </h1>
                <div class="is-size-5 publication-authors">
                  <!-- Paper authors -->
                  <span class="author-block">
                    <a href="https://qiancx.com/" target="_blank">Chengxuan Qian</a><sup>1,†</sup>,</span>
                  <span class="author-block">
                    <a href="https://shuoxing98.github.io/" target="_blank">Shuo Xing</a><sup>1</sup>,</span>
                  <span class="author-block">
                    <a href="https://lili0415.github.io/" target="_blank">Shawn Li</a><sup>2</sup>,
                  </span>
                  <span class="author-block">
                    <a href="https://viterbi-web.usc.edu/~yzhao010/lab" target="_blank">Yue Zhao</a><sup>2</sup>,
                  </span>
                  <span class="author-block">
                    <a href="https://vztu.github.io/" target="_blank">Zhengzhong Tu</a><sup>1,*</sup>,
                  </span>
                </div>
                  
                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Texas A&M University, <sup>2</sup>University of Southern California</span>
                    <span class="eql-cntrb"><small><br><sup>†</sup>Work done during the internship at Texas A&M University</small></span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Corresponding author</small></span>
                  </div>
                  
                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/taco-group/DecAlign" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2502.13146" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- <video poster="" id="tree" autoplay controls muted loop height="100%"> -->
        <!-- Your video here -->
        <!-- <source src="static/videos/banner_video.mp4" -->
        <!-- type="video/mp4"> -->
      <!-- </video> -->
      <img src="figs\decalign_pip.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        The Overview of our proposed DecAlign framework. 
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Multimodal representation learning aims to capture both shared and complementary semantic information across multiple modalities. However, the intrinsic heterogeneity of diverse modalities presents substantial challenges to achieve effective cross-modal collaboration and integration. To address this, we introduce DecAlign, a novel hierarchical cross-modal alignment framework designed to decouple multimodal representations into modality-unique (heterogeneous) and modality-common (homogeneous) features. For handling heterogeneity, we employ a prototype-guided optimal transport alignment strategy leveraging gaussian mixture modeling and multi-marginal transport plans, thus mitigating distribution discrepancies while preserving modality-unique characteristics. To reinforce homogeneity, we ensure semantic consistency across modalities by aligning latent distribution matching with Maximum Mean Discrepancy regularization. Furthermore, we incorporate a multimodal transformer to enhance high-level semantic feature fusion, thereby further reducing cross-modal inconsistencies. Our extensive experiments on four widely used multimodal benchmarks demonstrate that DecAlign consistently outperforms existing state-of-the-art methods across five metrics. These results highlight the efficacy of DecAlign in enhancing superior cross-modal alignment and semantic consistency while preserving modality-unique features, marking a significant advancement in multimodal representation learning scenarios.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop content">
      <h2 class="title is-3">Method Overview</h2>
      <img src="figs\overview.png" alt="MY ALT TEXT" style="display: block; margin: 0 auto; width: 800px; height: auto;"/>
      <div class="content has-text-justified">
        <p>

        </p>
        <ul>
          <li><strong>Multimodal Representation Decoupling: </strong>DecAlign decouples multimodal features into modality-unique (heterogeneous) and modality-common (homogeneous) representations. Modality-unique encoders extract heterogeneous features to capture specific characteristics, while a modality-common encoder derives homogeneous features for common semantics. This decoupling effectively reduces cross-modal interference while simultaneously enhancing the ability to capture underlying semantic commonalities across modalities.</li>
          <li><strong>Heterogeneity Alignment: </strong>DecAlign introduces a prototype-guided optimal transport strategy to mitigate distributional discrepancies among modality-unique features, which often pose challenges to seamless cross-modal integration. By incorporating Gaussian Mixture Models (GMM), DecAlign effectively captures complex intra-modal structures, generating adaptive prototypes that act as alignment anchors. A multi-marginal optimal transport mechanism then dynamically aligns these prototypes across modalities, bridging distributional gaps while preserving modality-unique characteristics. This approach not only maintains the distinctive nuances of each modality but also facilitates a more cohesive integration into a shared multimodal representation space.</li>
          <li><strong>Homogeneity Alignment: </strong>To ensure semantic consistency and enable seamless multimodal fusion, DecAlign aligns common features through latent distribution matching and maximum mean discrepancy regularization. By systematically correcting global statistical shifts in means, covariances, and higher-order moments, DecAlign preserves the intrinsic semantic relationships of modality-common representations while mitigating distortions caused by distributional inconsistencies. This approach not only fosters effective feature integration but also enhances the model’s robustness in handling diverse multimodal scenarios.</li>
          <li><strong>Hierarchical Alignment Strategy: </strong>DecAlign bridges modality gaps by first aligning heterogeneous features through prototype-based optimal transport, then ensuring semantic coherence via latent space alignment and MMD regularization. This hierarchical approach enhances multimodal fusion accuracy and generalizability, as demonstrated in benchmark evaluations.</li>
        </ul> 
      </div>
    </div>
  </div>
</section>
<!-- End image carousel -->


<!-- Image carousel -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container is-max-desktop content">
      <h2 class="title is-3">Comparison Analysis</h2>
        <p>
          Comprehensive experiments conducted across four widely-used multimodal benchmarks demonstrate DecAlign's superior performance compared to existing state-of-the-art methods. The results consistently show that DecAlign achieves substantial improvements in both fine-grained semantic distinction and overall alignment accuracy, highlighting the effectiveness and robustness of its hierarchical alignment strategy.
        </p>
        
        <img src="figs\table1.png" alt="Comparison 1" style="display: block; margin: 0 auto; width: 1000px; height: auto;"/>
        <p>
          The first evaluation focuses on object recognition accuracy within multimodal datasets. DecAlign significantly improves the precision of object classification by leveraging its hierarchical alignment approach, reducing misclassification rates by a notable margin.
        </p>
        
        <img src="asset/pics/image2.png" alt="Comparison 2" style="display: block; margin: 0 auto; width: 700px; height: auto;"/>
        <p>
          In the second benchmark, we evaluate cross-modal retrieval effectiveness. DecAlign outperforms competing models in retrieving semantically relevant text given an image input, demonstrating improved contextual awareness in vision-language tasks.
        </p>
        
        <img src="asset/pics/image3.png" alt="Comparison 3" style="display: block; margin: 0 auto; width: 700px; height: auto;"/>
        <p>
          The third test assesses alignment consistency between textual and visual representations. The results indicate that DecAlign reduces hallucination and enhances the coherence between generated text and associated images.
        </p>
        
        <img src="asset/pics/image4.png" alt="Comparison 4" style="display: block; margin: 0 auto; width: 700px; height: auto;"/>
        <p>
          Finally, we analyze robustness against noisy inputs. DecAlign maintains stable performance even in scenarios with ambiguous or incomplete information, making it highly reliable for real-world applications.
        </p>
      </div>
    </div>
  </div>
</section>
<!-- End image carousel -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop content">
      <h2 class="title is-3">Results</h2>
        <p>
          Re-Align achieves the best among the evaluated methods on both POPE and HallusionBench for LLaVA-v1.5-7B and LLaVA-v1.6-Mistral-7B, highlighting the effectiveness of our approach in mitigating hallucinations of VLMs. Furthermore, Re-Align can provide generally on-par or better performance than the vanilla models and baseline alignment methods on each evaluated general VQA task, ultimately achieving the best overall results.
        </p>
        <img src="asset\pics\table1.png" alt="MY ALT TEXT" style="display: block; margin: 0 auto; width: 800px; height: auto;"/>
        <div class="content has-text-centered"><p></p><strong>Table 1. </strong> 
          <em>Impact of Re-Align across hallucination benchmarks for VLMs, and comparisons with baselines.</em>
        </div>
        <img src="asset\pics\table2.png" alt="MY ALT TEXT" style="display: block; margin: 0 auto; width: 800px; height: auto;"/>
        <div class="content has-text-centered"><p></p><strong>Table 2. </strong> 
          <em>Impact of Re-Align across general benchmarks for VLMs, and comparisons with baselines.</em>
        </div>

      </div>
    </div>
  </div>
</section>
<!-- End image carousel -->

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>